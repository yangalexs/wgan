{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from dataset import RoadDataset, rotation_angles  # Keep your original import\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# Define Residual Transposed Convolution Block\n",
    "class ResUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.shortcut = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.up(x)\n",
    "        out = self.bn(out)\n",
    "        out += identity  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Generator\n",
    "class ConvGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, output_channels=4, output_height=128, output_width=128):\n",
    "        super(ConvGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512 * 8 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            ResUpBlock(512, 256),  # 16x16\n",
    "            ResUpBlock(256, 128),  # 32x32\n",
    "            ResUpBlock(128, 64),  # 64x64\n",
    "            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),  # 128x128\n",
    "            nn.Sigmoid()  # Add Sigmoid activation to constrain output to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.initial(z)\n",
    "        x = x.view(-1, 512, 8, 8)\n",
    "        out = self.main(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "class ConvDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=4, input_height=128, input_width=128):\n",
    "        super(ConvDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=8, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1)\n",
    "\n",
    "\n",
    "# Compute gradient penalty\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device)\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "# Improved visualization that shows physical dimensions\n",
    "def visualize_samples(real_data, fake_data, epoch, scaler, save_dir='visualizations'):\n",
    "    # Create directory for saving visualizations if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert from tensor to numpy\n",
    "    real_data = real_data[0].cpu().numpy()  # Shape: [4, 128, 128]\n",
    "    fake_data = fake_data[0].cpu().numpy()  # Shape: [4, 128, 128]\n",
    "    \n",
    "    # Get physical dimensions\n",
    "    x_width = 0.35  # 35 cm\n",
    "    y_length = 6.0  # 6 m\n",
    "    z_height = 0.25  # Adjust based on your data\n",
    "    \n",
    "    # Extract valid points\n",
    "    real_mask = ~np.isclose(real_data[2], 0)  # Non-zero Z values\n",
    "    fake_mask = ~np.isclose(fake_data[2], 0)  # Non-zero Z values\n",
    "    \n",
    "    # Flatten the spatial dimensions to get all points\n",
    "    real_x = real_data[0].flatten() * x_width\n",
    "    real_y = real_data[1].flatten() * y_length\n",
    "    real_z = real_data[2].flatten() * z_height\n",
    "    \n",
    "    fake_x = fake_data[0].flatten() * x_width\n",
    "    fake_y = fake_data[1].flatten() * y_length\n",
    "    fake_z = fake_data[2].flatten() * z_height\n",
    "    \n",
    "    # Create figure with 2 rows and 2 columns\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Row 1: Z vs Y\n",
    "    # Real data - Z vs Y\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(real_y, real_z, c='black', s=3, alpha=0.7)\n",
    "    plt.title(\"Real Data: Z vs Y\")\n",
    "    plt.xlabel(\"Y (meters)\")\n",
    "    plt.ylabel(\"Z (meters)\")\n",
    "    plt.xlim(0, y_length)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Fake data - Z vs Y\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(fake_y, fake_z, c='black', s=3, alpha=0.7)\n",
    "    plt.title(\"Generated Data: Z vs Y\")\n",
    "    plt.xlabel(\"Y (meters)\")\n",
    "    plt.ylabel(\"Z (meters)\")\n",
    "    plt.xlim(0, y_length)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Row 2: Y vs X\n",
    "    # Real data - Y vs X\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(real_x, real_y, c='black', s=3, alpha=0.7)\n",
    "    plt.title(\"Real Data: Y vs X\")\n",
    "    plt.xlabel(\"X (meters)\")\n",
    "    plt.ylabel(\"Y (meters)\")\n",
    "    plt.xlim(0, x_width)\n",
    "    plt.ylim(0, y_length)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Fake data - Y vs X\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(fake_x, fake_y, c='black', s=3, alpha=0.7)\n",
    "    plt.title(\"Generated Data: Y vs X\")\n",
    "    plt.xlabel(\"X (meters)\")\n",
    "    plt.ylabel(\"Y (meters)\")\n",
    "    plt.xlim(0, x_width)\n",
    "    plt.ylim(0, y_length)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Training Epoch {epoch}\")\n",
    "    plt.subplots_adjust(top=0.92)  # Adjust for the title\n",
    "    \n",
    "    # Save figure instead of showing it\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:04d}.png'), dpi=200)\n",
    "    plt.close()  # Close figure to free memory\n",
    "\n",
    "\n",
    "# Calculate SSIM and PSNR\n",
    "def compute_metrics(real_data, fake_data):\n",
    "    real_xyz = real_data[:, :3, :, :].detach().cpu().numpy()\n",
    "    fake_xyz = fake_data[:, :3, :, :].detach().cpu().numpy()\n",
    "\n",
    "    real_flat = real_xyz[0].reshape(3, -1).T\n",
    "    fake_flat = fake_xyz[0].reshape(3, -1).T\n",
    "\n",
    "    data_range = real_flat.max() - real_flat.min()\n",
    "    ssim_val = ssim(real_flat, fake_flat, channel_axis=1, data_range=data_range)\n",
    "    psnr_val = psnr(real_flat, fake_flat, data_range=data_range)\n",
    "    return ssim_val, psnr_val\n",
    "\n",
    "\n",
    "# Data augmentation function - MODIFIED FOR ~30 TOTAL SAMPLES\n",
    "def augment_data(normalized_data, augmentation_factor=5):\n",
    "    \"\"\"\n",
    "    Create multiple augmented versions of each road profile sample.\n",
    "    \n",
    "    Args:\n",
    "        normalized_data: Numpy array of shape [n_samples, 4, target_rows, target_points]\n",
    "        augmentation_factor: Number of augmented samples to create per original sample\n",
    "        \n",
    "    Returns:\n",
    "        Augmented dataset with approximately 30 total samples\n",
    "    \"\"\"\n",
    "    print(f\"Starting data augmentation: {len(normalized_data)} original samples\")\n",
    "    augmented_data = []\n",
    "    \n",
    "    # For each original sample\n",
    "    for sample in normalized_data:\n",
    "        # Add original sample\n",
    "        augmented_data.append(sample.copy())\n",
    "        \n",
    "        # 1. Add variations with bump height changes (reduced to 2)\n",
    "        for scale in [0.85, 1.15]:\n",
    "            # Identify the road baseline (minimum height)\n",
    "            z_values = sample[2]  # Z coordinates\n",
    "            baseline = np.min(z_values)\n",
    "            \n",
    "            # Scale only the height above baseline (the bump)\n",
    "            new_sample = sample.copy()\n",
    "            bump_height = z_values - baseline\n",
    "            new_sample[2] = baseline + bump_height * scale\n",
    "            augmented_data.append(new_sample)\n",
    "        \n",
    "        # 2. Add variations with bump position shifts (reduced to 2)\n",
    "        for shift in [-0.4, 0.4]:\n",
    "            new_sample = sample.copy()\n",
    "            z_values = sample[2]\n",
    "            \n",
    "            # Determine bump region (center of the road ~2.5-3.5m)\n",
    "            bump_center = int(128 * 3.0 / 6.0)  # Y position at 3m (middle of the road)\n",
    "            bump_width = int(128 * 0.8 / 6.0)   # Approx 0.8m width\n",
    "            \n",
    "            # Create a 1D Gaussian mask for smooth blending\n",
    "            y_indices = np.arange(128)\n",
    "            bump_mask = np.exp(-0.5 * ((y_indices - bump_center) / (bump_width/2))**2)\n",
    "            bump_mask = bump_mask / np.max(bump_mask)\n",
    "            \n",
    "            # Apply shift by creating a shifted version of the height\n",
    "            shift_pixels = int(shift / 6.0 * 128)  # Convert meters to pixels\n",
    "            shifted_z = np.zeros_like(z_values)\n",
    "            \n",
    "            # Apply shift while keeping within bounds\n",
    "            for i in range(128):\n",
    "                for j in range(128):\n",
    "                    new_i = i\n",
    "                    new_j = min(max(0, j + shift_pixels), 127)\n",
    "                    shifted_z[new_i, new_j] = max(shifted_z[new_i, new_j], z_values[i, j])\n",
    "            \n",
    "            # Blend original and shifted heights\n",
    "            new_sample[2] = shifted_z\n",
    "            augmented_data.append(new_sample)\n",
    "            \n",
    "        # 3. Add noise variation (reduced to 1)\n",
    "        new_sample = sample.copy()\n",
    "        z_noise = np.random.normal(0, 0.015, sample[2].shape)\n",
    "        new_sample[2] = np.clip(sample[2] + z_noise, 0, 1)  # Keep within [0,1]\n",
    "        augmented_data.append(new_sample)\n",
    "    \n",
    "    augmented_data = np.array(augmented_data)\n",
    "    print(f\"Completed data augmentation: {len(augmented_data)} total samples (target: ~30)\")\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "# Modified preprocessing to preserve road features\n",
    "def preprocess_data(pcd_files, target_rows=128, target_points=128, apply_augmentation=True):\n",
    "    dataset = RoadDataset(pcd_files, rotation_angles, target_rows, target_points)\n",
    "    all_data = []\n",
    "    \n",
    "    # From dictionary to 4D array\n",
    "    for sample in dataset:\n",
    "        data_list = sample['data']  # List of arrays, each with shape [4, num_points]\n",
    "        \n",
    "        # Convert to 4D array: [4, target_rows, target_points]\n",
    "        sample_data = np.zeros((4, target_rows, target_points))\n",
    "        for i, row in enumerate(data_list):\n",
    "            sample_data[:, i, :] = row\n",
    "        \n",
    "        all_data.append(sample_data)\n",
    "    \n",
    "    all_data = np.array(all_data)  # [n_samples, 4, target_rows, target_points]\n",
    "    \n",
    "    # Normalize each channel individually to preserve structure\n",
    "    n_samples = all_data.shape[0]\n",
    "    for s in range(n_samples):\n",
    "        for c in range(3):  # Only normalize X, Y, Z\n",
    "            min_val = np.min(all_data[s, c])\n",
    "            max_val = np.max(all_data[s, c])\n",
    "            if max_val > min_val:\n",
    "                all_data[s, c] = (all_data[s, c] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Apply data augmentation if requested\n",
    "    if apply_augmentation:\n",
    "        all_data = augment_data(all_data)\n",
    "    \n",
    "    return all_data, None  # Return None for scaler as we're not using it\n",
    "\n",
    "\n",
    "def spatial_coherence_loss(fake_data):\n",
    "    # Penalize scattered points\n",
    "    z_values = fake_data[:, 2]  # Height values [batch, 128, 128]\n",
    "    # Calculate local variation and penalize high variation\n",
    "    h_var = ((z_values[:, :, 1:] - z_values[:, :, :-1])**2).mean()\n",
    "    return h_var * 10.0  # Scaling factor\n",
    "\n",
    "\n",
    "def scan_line_loss(fake_data):\n",
    "    # Extract Y vs X data\n",
    "    y_values = fake_data[:, 1]  # Y coordinates [batch, 128, 128]\n",
    "    # Penalize vertical variation within rows\n",
    "    row_var = torch.tensor(0.0, device=fake_data.device)\n",
    "    for i in range(128):\n",
    "        row = y_values[:, i, :]\n",
    "        row_var += torch.var(row, dim=1).mean()\n",
    "    return row_var * 5.0\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    batch_size = 1\n",
    "    n_epochs = 5000\n",
    "    n_critic = 5\n",
    "    lambda_gp = 15\n",
    "    lr = 0.00002\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    base_dir = 'data_segment'  # Updated to use a relative path\n",
    "    pcd_files = glob(os.path.join(base_dir, '*.pcd'))\n",
    "    \n",
    "    # Visualization settings\n",
    "    save_dir = 'visualizations'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Will save visualizations every 50 epochs to: {os.path.abspath(save_dir)}\")\n",
    "\n",
    "    # Data preprocessing with improved normalization and augmentation\n",
    "    normalized_data, _ = preprocess_data(pcd_files, apply_augmentation=True)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    tensor_data = torch.tensor(normalized_data, dtype=torch.float32)\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize models\n",
    "    generator = ConvGenerator(latent_dim).to(device)\n",
    "    discriminator = ConvDiscriminator().to(device)\n",
    "\n",
    "    # Optimizers with weight decay\n",
    "    optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=1e-5)\n",
    "    optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=1e-5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (real_data,) in enumerate(dataloader):  # Unpack single tensor\n",
    "            real_data = real_data.to(device)  # Already [batch, 4, 128, 128]\n",
    "\n",
    "            # Train discriminator\n",
    "            for _ in range(n_critic):\n",
    "                optim_d.zero_grad()\n",
    "                z = torch.randn(real_data.size(0), latent_dim).to(device)  # Match batch size\n",
    "                fake_data = generator(z)\n",
    "                real_validity = discriminator(real_data)\n",
    "                fake_validity = discriminator(fake_data)\n",
    "                gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data, device)\n",
    "\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "                d_loss.backward()\n",
    "                optim_d.step()\n",
    "\n",
    "            # Train generator\n",
    "            optim_g.zero_grad()\n",
    "            z = torch.randn(real_data.size(0), latent_dim).to(device)  # Match batch size\n",
    "            fake_data = generator(z)\n",
    "            fake_validity = discriminator(fake_data)\n",
    "\n",
    "            # Calculate custom losses\n",
    "            coherence_loss = spatial_coherence_loss(fake_data)\n",
    "            line_loss = scan_line_loss(fake_data)\n",
    "            \n",
    "            # Combined loss\n",
    "            g_loss = -torch.mean(fake_validity) + coherence_loss + line_loss    \n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "\n",
    "            # Calculate SSIM and PSNR\n",
    "            if i == 0:\n",
    "                ssim_val, psnr_val = compute_metrics(real_data, fake_data)\n",
    "\n",
    "        # Print loss and metrics\n",
    "        print(f\"Training Epoch [{epoch}/{n_epochs}] D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}, \"\n",
    "              f\"SSIM: {ssim_val:.4f}, PSNR: {psnr_val:.4f}\")\n",
    "\n",
    "        # Visualize and save\n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(1, latent_dim).to(device)\n",
    "                fake_data = generator(z)\n",
    "                visualize_samples(real_data[:1], fake_data, epoch, None, save_dir)\n",
    "                \n",
    "    # Save final models\n",
    "    torch.save(generator.state_dict(), os.path.join(save_dir, 'generator_final.pth'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(save_dir, 'discriminator_final.pth'))\n",
    "    print(f\"Training complete! Final models and visualizations saved to: {os.path.abspath(save_dir)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
